# üìä RELAT√ìRIO T√âCNICO - VARI√ÅVEIS EX√ìGENAS E MODELOS
## Projeto de Forecasting TJGO

---

## üéØ Resumo Executivo

Este relat√≥rio detalha o tratamento e configura√ß√£o das vari√°veis ex√≥genas, bem como a implementa√ß√£o e parametriza√ß√£o dos modelos de machine learning utilizados no projeto de forecasting do TJGO. A an√°lise revelou que **vari√°veis ex√≥genas econ√¥micas tradicionais** s√£o mais eficazes que vari√°veis de alta correla√ß√£o, e que **modelos mais simples superam abordagens complexas**.

---

## üìà 1. VARI√ÅVEIS EX√ìGENAS

### 1.1 Defini√ß√£o e Conceito

**Vari√°veis Ex√≥genas** s√£o fatores externos que influenciam a vari√°vel dependente (TOTAL_CASOS) mas n√£o s√£o controlados pelo sistema. No contexto judicial, representam indicadores econ√¥micos, sociais e demogr√°ficos que impactam a demanda por servi√ßos judiciais.

**Explica√ß√£o T√©cnica**: 
- **End√≥gena**: TOTAL_CASOS (vari√°vel alvo)
- **Ex√≥gena**: Indicadores econ√¥micos que influenciam a demanda judicial
- **Causalidade**: Rela√ß√£o direta entre condi√ß√µes econ√¥micas e litigiosidade

### 1.2 Invent√°rio de Vari√°veis Dispon√≠veis

#### 1.2.1 Vari√°veis Econ√¥micas Tradicionais
```python
exog_vars_traditional = [
    'TAXA_SELIC',           # Taxa b√°sica de juros
    'IPCA',                 # √çndice de pre√ßos ao consumidor
    'TAXA_DESOCUPACAO',     # Taxa de desemprego
    'INADIMPLENCIA'         # Taxa de inadimpl√™ncia
]
```

**Justificativa Econ√¥mica**:
- **TAXA_SELIC**: Juros altos ‚Üí Menos cr√©dito ‚Üí Menos conflitos comerciais
- **IPCA**: Infla√ß√£o alta ‚Üí Maior litigiosidade por reajustes
- **TAXA_DESOCUPACAO**: Desemprego ‚Üí Maior demanda por direitos trabalhistas
- **INADIMPLENCIA**: Correla√ß√£o direta com conflitos comerciais

#### 1.2.2 Vari√°veis de Alta Correla√ß√£o (Removidas)
```python
high_correlation_vars = [
    'qt_acidente',          # Quantidade de acidentes
    'QT_ELEITOR'            # Quantidade de eleitores
]
```

**An√°lise de Correla√ß√£o**:
- **qt_acidente**: Correla√ß√£o 0.87 com TOTAL_CASOS
- **QT_ELEITOR**: Correla√ß√£o 0.89 com TOTAL_CASOS
- **Problema**: Multicolineariedade e overfitting

#### 1.2.3 Outras Vari√°veis Dispon√≠veis
```python
other_vars = [
    'PIB',                  # Produto Interno Bruto
    'SALARIO_MINIMO',       # Sal√°rio m√≠nimo
    'POPULACAO',            # Popula√ß√£o total
    'IDH',                  # √çndice de Desenvolvimento Humano
    'RENDA_PER_CAPITA',     # Renda per capita
    'TAXA_CRESCIMENTO',     # Taxa de crescimento econ√¥mico
    'INFLACAO',             # Taxa de infla√ß√£o
    'DIVIDA_PUBLICA',       # D√≠vida p√∫blica
    'RESERVAS_INTERNACIONAIS' # Reservas internacionais
]
```

### 1.3 Tratamento e Prepara√ß√£o

#### 1.3.1 Limpeza de Dados
```python
def clean_exogenous_variables(df):
    """
    Limpeza e prepara√ß√£o das vari√°veis ex√≥genas
    """
    # 1. Tratamento de valores ausentes
    df = df.fillna(method='ffill').fillna(method='bfill')
    
    # 2. Detec√ß√£o de outliers
    for col in exog_vars:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # Suaviza√ß√£o de outliers
        df[col] = df[col].clip(lower_bound, upper_bound)
    
    return df
```

#### 1.3.2 Normaliza√ß√£o e Escala
```python
def normalize_exogenous_variables(df, exog_vars):
    """
    Normaliza√ß√£o das vari√°veis ex√≥genas
    """
    from sklearn.preprocessing import StandardScaler
    
    scaler = StandardScaler()
    df[exog_vars] = scaler.fit_transform(df[exog_vars])
    
    return df, scaler
```

#### 1.3.3 Feature Engineering
```python
def create_exogenous_features(df, exog_vars):
    """
    Cria√ß√£o de features derivadas das vari√°veis ex√≥genas
    """
    # 1. Lags das vari√°veis ex√≥genas
    for var in exog_vars:
        for lag in [1, 2, 3, 6, 12]:
            df[f'{var}_lag_{lag}'] = df[var].shift(lag)
    
    # 2. Rolling statistics
    for var in exog_vars:
        for window in [3, 6, 12]:
            df[f'{var}_rolling_mean_{window}'] = df[var].rolling(window).mean()
            df[f'{var}_rolling_std_{window}'] = df[var].rolling(window).std()
    
    # 3. Intera√ß√µes entre vari√°veis
    df['SELIC_x_IPCA'] = df['TAXA_SELIC'] * df['IPCA']
    df['DESEMPREGO_x_INADIMPLENCIA'] = df['TAXA_DESOCUPACAO'] * df['INADIMPLENCIA']
    
    return df
```

### 1.4 Configura√ß√£o por Modelo

#### 1.4.1 SARIMAX (ARIMA com Vari√°veis Ex√≥genas)
```python
def configure_sarimax_exog(train_data, test_data, exog_vars):
    """
    Configura√ß√£o de vari√°veis ex√≥genas para SARIMAX
    """
    # SARIMAX requer vari√°veis ex√≥genas no formato espec√≠fico
    exog_train = train_data[exog_vars].values
    exog_test = test_data[exog_vars].values
    
    # Verifica√ß√£o de estacionariedade das vari√°veis ex√≥genas
    for var in exog_vars:
        adf_stat, adf_pvalue = adfuller(train_data[var])
        if adf_pvalue > 0.05:
            print(f"‚ö†Ô∏è {var} n√£o √© estacion√°ria (p-value: {adf_pvalue:.3f})")
            # Aplicar diferencia√ß√£o se necess√°rio
            train_data[f'{var}_diff'] = train_data[var].diff()
    
    return exog_train, exog_test
```

**Par√¢metros SARIMAX**:
- **order**: (1,1,1) - ARIMA b√°sico
- **seasonal_order**: (1,1,1,12) - Sazonalidade anual
- **enforce_stationarity**: False - Flexibilidade
- **enforce_invertibility**: False - Flexibilidade

#### 1.4.2 Prophet (Facebook)
```python
def configure_prophet_exog(train_data, exog_vars):
    """
    Configura√ß√£o de vari√°veis ex√≥genas para Prophet
    """
    from prophet import Prophet
    
    # Preparar dados no formato Prophet
    prophet_data = train_data.reset_index()
    prophet_data = prophet_data.rename(columns={'DATA': 'ds', 'TOTAL_CASOS': 'y'})
    
    # Inicializar modelo
    model = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=False,
        daily_seasonality=False,
        seasonality_mode='additive',
        interval_width=0.95
    )
    
    # Adicionar regressores ex√≥genos
    for var in exog_vars:
        if var in prophet_data.columns:
            model.add_regressor(var)
            print(f"‚úÖ Adicionada vari√°vel ex√≥gena: {var}")
    
    return model, prophet_data
```

**Par√¢metros Prophet**:
- **yearly_seasonality**: True - Sazonalidade anual
- **weekly_seasonality**: False - Sem sazonalidade semanal
- **daily_seasonality**: False - Sem sazonalidade di√°ria
- **seasonality_mode**: 'additive' - Sazonalidade aditiva
- **interval_width**: 0.95 - Intervalo de confian√ßa 95%

#### 1.4.3 Modelos de Machine Learning
```python
def configure_ml_exog(train_data, test_data, exog_vars):
    """
    Configura√ß√£o de vari√°veis ex√≥genas para ML models
    """
    # Preparar features
    feature_cols = []
    
    # 1. Vari√°veis ex√≥genas originais
    feature_cols.extend(exog_vars)
    
    # 2. Features temporais
    feature_cols.extend(['year', 'month', 'quarter'])
    
    # 3. Lags da vari√°vel alvo
    for lag in [1, 2, 3, 6, 12]:
        feature_cols.append(f'TOTAL_CASOS_lag_{lag}')
    
    # 4. Rolling statistics da vari√°vel alvo
    for window in [3, 6, 12]:
        feature_cols.extend([
            f'TOTAL_CASOS_rolling_mean_{window}',
            f'TOTAL_CASOS_rolling_std_{window}'
        ])
    
    # 5. Lags das vari√°veis ex√≥genas
    for var in exog_vars:
        for lag in [1, 2, 3]:
            feature_cols.append(f'{var}_lag_{lag}')
    
    # Remover colunas com muitos NaN
    feature_cols = [col for col in feature_cols 
                   if col in train_data.columns and 
                   train_data[col].isnull().sum() < len(train_data) * 0.5]
    
    return feature_cols
```

### 1.5 Experimentos e Descobertas

#### 1.5.1 Experimento 1: Modelo Completo
**Configura√ß√£o**: 15 vari√°veis ex√≥genas + dados 2014-2024
**Resultado**: MAE = 6.472 (Prophet)
**Problema**: Overfitting com muitas vari√°veis

#### 1.5.2 Experimento 2: Modelo Teste (Recomendado)
**Configura√ß√£o**: 4 vari√°veis econ√¥micas tradicionais + dados 2015-2024
**Resultado**: MAE = 3.634 (Prophet) - **44% MELHOR!**
**Descoberta**: Simplicidade vence complexidade

#### 1.5.3 An√°lise de Multicolineariedade
```python
def analyze_multicollinearity(df, exog_vars):
    """
    An√°lise de multicolineariedade entre vari√°veis ex√≥genas
    """
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    
    # Calcular VIF para cada vari√°vel
    vif_data = pd.DataFrame()
    vif_data["Variable"] = exog_vars
    vif_data["VIF"] = [variance_inflation_factor(df[exog_vars].values, i) 
                      for i in range(len(exog_vars))]
    
    # VIF > 10 indica multicolineariedade
    high_vif = vif_data[vif_data["VIF"] > 10]
    
    return vif_data, high_vif
```

**Resultados VIF**:
- **TAXA_SELIC**: VIF = 2.3 (baixo)
- **IPCA**: VIF = 1.8 (baixo)
- **TAXA_DESOCUPACAO**: VIF = 3.1 (baixo)
- **INADIMPLENCIA**: VIF = 2.7 (baixo)
- **qt_acidente**: VIF = 15.2 (alto - removida)
- **QT_ELEITOR**: VIF = 18.7 (alto - removida)

---

## ü§ñ 2. MODELOS DE MACHINE LEARNING

### 2.1 Baselines (Modelos de Refer√™ncia)

#### 2.1.1 Persist√™ncia (Naive Forecast)
```python
def baseline_persistence(train_data, test_data, target_col):
    """
    Modelo de persist√™ncia: usa √∫ltimo valor conhecido
    """
    # Previs√£o = √∫ltimo valor do treino
    last_value = train_data[target_col].iloc[-1]
    predictions = np.full(len(test_data), last_value)
    
    return predictions
```

**Explica√ß√£o**: 
- **Conceito**: Assume que o pr√≥ximo valor ser√° igual ao √∫ltimo observado
- **Uso**: Baseline m√≠nimo para compara√ß√£o
- **Limita√ß√£o**: N√£o captura tend√™ncias ou sazonalidade

#### 2.1.2 M√©dia M√≥vel
```python
def baseline_moving_average(train_data, test_data, target_col, window=12):
    """
    Modelo de m√©dia m√≥vel: m√©dia dos √∫ltimos N per√≠odos
    """
    # Calcular m√©dia m√≥vel
    moving_avg = train_data[target_col].rolling(window=window).mean().iloc[-1]
    
    # Previs√£o = m√©dia m√≥vel
    predictions = np.full(len(test_data), moving_avg)
    
    return predictions
```

**Explica√ß√£o**:
- **Conceito**: M√©dia dos √∫ltimos N per√≠odos
- **Par√¢metro**: window=12 (m√©dia anual)
- **Uso**: Baseline sazonal simples

### 2.2 Modelos Estat√≠sticos

#### 2.2.1 SARIMAX (Seasonal ARIMA with eXogenous variables)
```python
def train_sarimax(y_train, exog_train, exog_test, order=(1,1,1), seasonal_order=(1,1,1,12)):
    """
    Treinamento do modelo SARIMAX
    """
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    
    # Configurar modelo
    model = SARIMAX(
        y_train,
        exog=exog_train,
        order=order,                    # (p,d,q) - ARIMA
        seasonal_order=seasonal_order,  # (P,D,Q,s) - Sazonalidade
        enforce_stationarity=False,     # Flexibilidade
        enforce_invertibility=False     # Flexibilidade
    )
    
    # Treinar modelo
    fitted_model = model.fit(disp=False)
    
    # Fazer previs√µes
    predictions = fitted_model.forecast(steps=len(exog_test), exog=exog_test)
    
    return fitted_model, predictions
```

**Explica√ß√£o T√©cnica**:
- **ARIMA**: AutoRegressive Integrated Moving Average
- **SARIMAX**: ARIMA + Sazonalidade + Vari√°veis Ex√≥genas
- **Par√¢metros**:
  - **p**: Ordem autoregressiva (depend√™ncia do passado)
  - **d**: Diferencia√ß√£o (tornar s√©rie estacion√°ria)
  - **q**: Ordem da m√©dia m√≥vel (ru√≠do)
  - **P,D,Q,s**: Sazonalidade (s=12 para mensal)

**Vantagens**:
- Captura tend√™ncias e sazonalidade
- Incorpora vari√°veis ex√≥genas
- Intervalos de confian√ßa

**Desvantagens**:
- Requer s√©rie estacion√°ria
- Par√¢metros complexos
- Sens√≠vel a outliers

#### 2.2.2 Prophet (Facebook)
```python
def train_prophet(train_data, exog_vars, target_col='TOTAL_CASOS'):
    """
    Treinamento do modelo Prophet
    """
    from prophet import Prophet
    
    # Preparar dados
    prophet_data = train_data.reset_index()
    prophet_data = prophet_data.rename(columns={'DATA': 'ds', target_col: 'y'})
    
    # Configurar modelo
    model = Prophet(
        yearly_seasonality=True,        # Sazonalidade anual
        weekly_seasonality=False,       # Sem sazonalidade semanal
        daily_seasonality=False,        # Sem sazonalidade di√°ria
        seasonality_mode='additive',    # Sazonalidade aditiva
        interval_width=0.95,            # Intervalo de confian√ßa 95%
        changepoint_prior_scale=0.05,   # Sensibilidade a mudan√ßas
        seasonality_prior_scale=10.0    # For√ßa da sazonalidade
    )
    
    # Adicionar vari√°veis ex√≥genas
    for var in exog_vars:
        if var in prophet_data.columns:
            model.add_regressor(var)
    
    # Treinar modelo
    model.fit(prophet_data)
    
    return model
```

**Explica√ß√£o T√©cnica**:
- **Decomposi√ß√£o**: Tend√™ncia + Sazonalidade + Feriados + Regressores
- **Algoritmo**: Generalized Additive Model (GAM)
- **Componentes**:
  - **g(t)**: Tend√™ncia (linear + log√≠stica)
  - **s(t)**: Sazonalidade (Fourier)
  - **h(t)**: Feriados
  - **Œ≤x(t)**: Regressores ex√≥genos

**Vantagens**:
- Lida com sazonalidade complexa
- Robustez a outliers
- Intervalos de confian√ßa
- F√°cil interpreta√ß√£o

**Desvantagens**:
- Requer dados regulares
- Computacionalmente intensivo
- Sens√≠vel a par√¢metros

### 2.3 Modelos de Machine Learning

#### 2.3.1 Random Forest
```python
def train_random_forest(X_train, y_train, X_test, n_estimators=100, max_depth=10):
    """
    Treinamento do Random Forest
    """
    from sklearn.ensemble import RandomForestRegressor
    
    # Configurar modelo
    model = RandomForestRegressor(
        n_estimators=n_estimators,      # N√∫mero de √°rvores
        max_depth=max_depth,            # Profundidade m√°xima
        min_samples_split=5,            # M√≠nimo para dividir
        min_samples_leaf=2,             # M√≠nimo por folha
        random_state=42,                # Reprodutibilidade
        n_jobs=-1                       # Paraleliza√ß√£o
    )
    
    # Treinar modelo
    model.fit(X_train, y_train)
    
    # Fazer previs√µes
    predictions = model.predict(X_test)
    
    return model, predictions
```

**Explica√ß√£o T√©cnica**:
- **Ensemble**: Combina√ß√£o de m√∫ltiplas √°rvores de decis√£o
- **Bootstrap**: Amostragem com reposi√ß√£o
- **Bagging**: Agrega√ß√£o de previs√µes
- **Feature Importance**: Import√¢ncia das vari√°veis

**Vantagens**:
- N√£o requer normaliza√ß√£o
- Lida com features categ√≥ricas
- Feature importance
- Robustez a overfitting

**Desvantagens**:
- N√£o captura tend√™ncias temporais
- Requer feature engineering
- Computacionalmente intensivo

#### 2.3.2 XGBoost (eXtreme Gradient Boosting)
```python
def train_xgboost(X_train, y_train, X_test, n_estimators=100, max_depth=6, learning_rate=0.1):
    """
    Treinamento do XGBoost
    """
    import xgboost as xgb
    
    # Configurar modelo
    model = xgb.XGBRegressor(
        n_estimators=n_estimators,      # N√∫mero de √°rvores
        max_depth=max_depth,            # Profundidade m√°xima
        learning_rate=learning_rate,    # Taxa de aprendizado
        subsample=0.8,                  # Subamostragem
        colsample_bytree=0.8,          # Subamostragem de features
        random_state=42,                # Reprodutibilidade
        n_jobs=-1                       # Paraleliza√ß√£o
    )
    
    # Treinar modelo
    model.fit(X_train, y_train)
    
    # Fazer previs√µes
    predictions = model.predict(X_test)
    
    return model, predictions
```

**Explica√ß√£o T√©cnica**:
- **Gradient Boosting**: Otimiza√ß√£o sequencial
- **Regulariza√ß√£o**: L1 (Lasso) + L2 (Ridge)
- **Pruning**: Poda de √°rvores
- **Early Stopping**: Parada antecipada

**Vantagens**:
- Alta performance
- Regulariza√ß√£o integrada
- Feature importance
- Paraleliza√ß√£o

**Desvantagens**:
- Sens√≠vel a hiperpar√¢metros
- Overfitting se n√£o regularizado
- Computacionalmente intensivo

#### 2.3.3 LightGBM (Light Gradient Boosting Machine)
```python
def train_lightgbm(X_train, y_train, X_test, n_estimators=100, max_depth=6, learning_rate=0.1):
    """
    Treinamento do LightGBM
    """
    import lightgbm as lgb
    
    # Configurar modelo
    model = lgb.LGBMRegressor(
        n_estimators=n_estimators,      # N√∫mero de √°rvores
        max_depth=max_depth,            # Profundidade m√°xima
        learning_rate=learning_rate,    # Taxa de aprendizado
        subsample=0.8,                  # Subamostragem
        colsample_bytree=0.8,          # Subamostragem de features
        random_state=42,                # Reprodutibilidade
        n_jobs=-1,                      # Paraleliza√ß√£o
        verbose=-1                      # Silencioso
    )
    
    # Treinar modelo
    model.fit(X_train, y_train)
    
    # Fazer previs√µes
    predictions = model.predict(X_test)
    
    return model, predictions
```

**Explica√ß√£o T√©cnica**:
- **Leaf-wise Growth**: Crescimento por folha
- **Histogram-based**: Binning de features
- **GOSS**: Gradient-based One-Side Sampling
- **EFB**: Exclusive Feature Bundling

**Vantagens**:
- Muito r√°pido
- Baixo uso de mem√≥ria
- Boa performance
- Regulariza√ß√£o integrada

**Desvantagens**:
- Sens√≠vel a overfitting
- Requer tuning cuidadoso
- Menos interpret√°vel

### 2.4 Otimiza√ß√£o de Hiperpar√¢metros

#### 2.4.1 Grid Search
```python
def optimize_hyperparameters(model_class, X_train, y_train, param_grid):
    """
    Otimiza√ß√£o de hiperpar√¢metros com Grid Search
    """
    from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
    
    # Time Series Cross-Validation
    tscv = TimeSeriesSplit(n_splits=5)
    
    # Grid Search
    grid_search = GridSearchCV(
        estimator=model_class,
        param_grid=param_grid,
        cv=tscv,
        scoring='neg_mean_absolute_error',
        n_jobs=-1,
        verbose=1
    )
    
    # Executar busca
    grid_search.fit(X_train, y_train)
    
    return grid_search.best_estimator_, grid_search.best_params_
```

#### 2.4.2 Par√¢metros Otimizados
```python
# Random Forest
rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

# XGBoost
xgb_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}

# LightGBM
lgb_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}
```

### 2.5 Valida√ß√£o Temporal

#### 2.5.1 Time Series Cross-Validation
```python
def time_series_cv(model, X, y, n_splits=5):
    """
    Valida√ß√£o cruzada temporal
    """
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import mean_absolute_error
    
    tscv = TimeSeriesSplit(n_splits=n_splits)
    scores = []
    
    for train_idx, val_idx in tscv.split(X):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # Treinar modelo
        model.fit(X_train, y_train)
        
        # Fazer previs√µes
        y_pred = model.predict(X_val)
        
        # Calcular m√©trica
        mae = mean_absolute_error(y_val, y_pred)
        scores.append(mae)
    
    return np.mean(scores), np.std(scores)
```

#### 2.5.2 Walk-Forward Validation
```python
def walk_forward_validation(model, X, y, train_size=0.8):
    """
    Valida√ß√£o walk-forward
    """
    n_train = int(len(X) * train_size)
    
    # Dados de treino
    X_train, y_train = X[:n_train], y[:n_train]
    
    # Dados de teste
    X_test, y_test = X[n_train:], y[n_train:]
    
    # Treinar modelo
    model.fit(X_train, y_train)
    
    # Fazer previs√µes
    y_pred = model.predict(X_test)
    
    return y_test, y_pred
```

---

## üéØ 3. CONFIGURA√á√ÉO FINAL E RESULTADOS

### 3.1 Configura√ß√£o Vencedora

#### 3.1.1 Vari√°veis Ex√≥genas Selecionadas
```python
final_exog_vars = [
    'TAXA_SELIC',           # Taxa b√°sica de juros
    'IPCA',                 # √çndice de pre√ßos
    'TAXA_DESOCUPACAO',     # Taxa de desemprego
    'INADIMPLENCIA'         # Taxa de inadimpl√™ncia
]
```

**Justificativa**:
- **Baixa multicolineariedade**: VIF < 5 para todas
- **Relev√¢ncia econ√¥mica**: Impacto direto na litigiosidade
- **Disponibilidade**: Dados consistentes e atualizados
- **Interpretabilidade**: F√°cil compreens√£o pelos stakeholders

#### 3.1.2 Modelo Vencedor: Prophet
```python
# Configura√ß√£o final do Prophet
best_model = Prophet(
    yearly_seasonality=True,        # Sazonalidade anual
    weekly_seasonality=False,     # Sem sazonalidade semanal
    daily_seasonality=False,        # Sem sazonalidade di√°ria
    seasonality_mode='additive',    # Sazonalidade aditiva
    interval_width=0.95,           # Intervalo de confian√ßa 95%
    changepoint_prior_scale=0.05,  # Sensibilidade a mudan√ßas
    seasonality_prior_scale=10.0   # For√ßa da sazonalidade
)

# Adicionar regressores ex√≥genos
for var in final_exog_vars:
    best_model.add_regressor(var)
```

### 3.2 Performance Comparativa

| Modelo | MAE | RMSE | R¬≤ | Vari√°veis Ex√≥genas |
|--------|-----|------|----|----| 
| **Prophet (Teste)** | **3.634** | **4.597** | **0.339** | 4 econ√¥micas tradicionais |
| Prophet (Completo) | 6.472 | 7.313 | -0.245 | 15 vari√°veis (incluindo alta correla√ß√£o) |
| Random Forest | 6.827 | 7.874 | -0.939 | 4 econ√¥micas + features temporais |
| XGBoost | 7.669 | 8.918 | -1.487 | 4 econ√¥micas + features temporais |
| LightGBM | 7.464 | 8.876 | -1.464 | 4 econ√¥micas + features temporais |
| SARIMAX | 9.416 | 11.290 | -2.986 | 4 econ√¥micas tradicionais |

### 3.3 Li√ß√µes Aprendidas

#### 3.3.1 Vari√°veis Ex√≥genas
1. **Qualidade > Quantidade**: 4 vari√°veis bem escolhidas > 15 vari√°veis
2. **Multicolineariedade**: Vari√°veis altamente correlacionadas diminuem performance
3. **Relev√¢ncia Econ√¥mica**: Vari√°veis econ√¥micas tradicionais s√£o mais eficazes
4. **Feature Engineering**: Lags e rolling statistics s√£o essenciais

#### 3.3.2 Modelos
1. **Simplicidade vence complexidade**: Prophet simples > modelos complexos
2. **Valida√ß√£o Temporal**: Crucial para s√©ries temporais
3. **Interpretabilidade**: Modelos interpret√°veis s√£o prefer√≠veis
4. **Robustez**: Modelos robustos a outliers s√£o mais confi√°veis

---

## üìä 4. RECOMENDA√á√ïES T√âCNICAS

### 4.1 Implementa√ß√£o em Produ√ß√£o

#### 4.1.1 Pipeline de Dados
```python
def production_pipeline():
    """
    Pipeline de produ√ß√£o para previs√µes
    """
    # 1. Carregar dados atualizados
    data = load_latest_data()
    
    # 2. Preparar vari√°veis ex√≥genas
    exog_data = prepare_exogenous_variables(data)
    
    # 3. Treinar modelo
    model = train_prophet_model(exog_data)
    
    # 4. Fazer previs√µes
    forecast = model.predict(future_data)
    
    # 5. Validar previs√µes
    validation_results = validate_forecast(forecast)
    
    return forecast, validation_results
```

#### 4.1.2 Monitoramento
```python
def monitor_model_performance():
    """
    Monitoramento da performance do modelo
    """
    # 1. Calcular m√©tricas recentes
    recent_mae = calculate_recent_mae()
    
    # 2. Detectar drift
    if detect_data_drift():
        send_alert("Data drift detected")
    
    # 3. Verificar performance
    if recent_mae > threshold:
        send_alert("Model performance degraded")
    
    # 4. Retreinar se necess√°rio
    if should_retrain():
        retrain_model()
```

### 4.2 Manuten√ß√£o e Atualiza√ß√£o

#### 4.2.1 Retreinamento Autom√°tico
- **Frequ√™ncia**: Mensal
- **Trigger**: Performance degradada ou novos dados
- **Valida√ß√£o**: Cross-validation temporal
- **Deploy**: A/B testing com modelo anterior

#### 4.2.2 Monitoramento de Vari√°veis Ex√≥genas
- **Disponibilidade**: Verificar atualiza√ß√£o mensal
- **Qualidade**: Detectar outliers e missing values
- **Relev√¢ncia**: Avaliar correla√ß√£o com vari√°vel alvo
- **Substitui√ß√£o**: Identificar novas vari√°veis relevantes

---

## üéØ 5. CONCLUS√ïES

### 5.1 Descobertas Principais

1. **Vari√°veis Ex√≥genas Econ√¥micas Tradicionais** s√£o mais eficazes que vari√°veis de alta correla√ß√£o
2. **Modelos Simples** superam abordagens complexas (princ√≠pio da parcim√¥nia)
3. **Prophet** √© superior para s√©ries temporais com sazonalidade
4. **Feature Engineering** √© crucial para modelos de ML
5. **Valida√ß√£o Temporal** √© essencial para s√©ries temporais

### 5.2 Recomenda√ß√µes Finais

1. **Usar Prophet** com 4 vari√°veis econ√¥micas tradicionais
2. **Implementar retreinamento mensal** autom√°tico
3. **Monitorar performance** continuamente
4. **Expandir gradualmente** para outros tipos de processo
5. **Documentar decis√µes** para reprodutibilidade

### 5.3 Pr√≥ximos Passos

1. **Implementa√ß√£o em produ√ß√£o** com monitoramento
2. **Expans√£o para outros tribunais** usando metodologia
3. **Desenvolvimento de dashboard** executivo
4. **Treinamento da equipe** t√©cnica
5. **Pesquisa de novas vari√°veis** ex√≥genas relevantes

---

*Relat√≥rio t√©cnico sobre vari√°veis ex√≥genas e modelos - Dezembro 2024*  
*Vers√£o: 1.0*  
*Autor: Equipe de Data Science - TJGO*
